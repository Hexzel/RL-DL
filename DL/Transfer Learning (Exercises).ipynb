{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning\n",
    "\n",
    "In this notebook, you'll learn how to use pre-trained networks to solved challenging problems in computer vision. Specifically, you'll use networks trained on [ImageNet](http://www.image-net.org/) [available from torchvision](http://pytorch.org/docs/0.3.0/torchvision/models.html). \n",
    "\n",
    "ImageNet is a massive dataset with over 1 million labeled images in 1000 categories. It's used to train deep neural networks using an architecture called convolutional layers. I'm not going to get into the details of convolutional networks here, but if you want to learn more about them, please [watch this](https://www.youtube.com/watch?v=2-Ol7ZB0MmU).\n",
    "\n",
    "Once trained, these models work astonishingly well as feature detectors for images they weren't trained on. Using a pre-trained network on images not in the training set is called transfer learning. Here we'll use transfer learning to train a network that can classify our cat and dog photos with near perfect accuracy.\n",
    "\n",
    "With `torchvision.models` you can download these pre-trained networks and use them in your applications. We'll include `models` in our imports now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms, models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the pretrained models require the input to be 224x224 images. Also, we'll need to match the normalization used when the models were trained. Each color channel was normalized separately, the means are `[0.485, 0.456, 0.406]` and the standard deviations are `[0.229, 0.224, 0.225]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'D:\\DataSet\\Pytorch\\Cat_Dog_data'\n",
    "\n",
    "# TODO: Define transforms for the training data and testing data\n",
    "train_transforms = transforms.Compose([ \n",
    "                                       transforms.RandomRotation(30),\n",
    "                                       transforms.RandomResizedCrop(224),\n",
    "                                       transforms.RandomHorizontalFlip(),\n",
    "                                       transforms.ToTensor(),\n",
    "                                       transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "                                     ])\n",
    "\n",
    "test_transforms = transforms.Compose([\n",
    "                                      transforms.CenterCrop(224),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "                                    ])\n",
    "\n",
    "# Pass transforms in here, then run the next cell to see how the transforms look\n",
    "train_data = datasets.ImageFolder(data_dir + '/train', transform=train_transforms)\n",
    "test_data = datasets.ImageFolder(data_dir + '/test', transform=test_transforms)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader(test_data, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can load in a model such as [DenseNet](http://pytorch.org/docs/0.3.0/torchvision/models.html#id5). Let's print out the model architecture so we can see what's going on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseNet(\n",
       "  (features): Sequential(\n",
       "    (conv0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (norm0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu0): ReLU(inplace=True)\n",
       "    (pool0): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (denseblock1): _DenseBlock(\n",
       "      (denselayer1): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer2): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(96, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer3): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer4): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(160, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer5): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer6): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(224, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (transition1): _Transition(\n",
       "      (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "    )\n",
       "    (denseblock2): _DenseBlock(\n",
       "      (denselayer1): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer2): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(160, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer3): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer4): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(224, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer5): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer6): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(288, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer7): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(320, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer8): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(352, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer9): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer10): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(416, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(416, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer11): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(448, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer12): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(480, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (transition2): _Transition(\n",
       "      (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "    )\n",
       "    (denseblock3): _DenseBlock(\n",
       "      (denselayer1): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer2): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(288, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer3): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(320, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer4): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(352, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer5): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer6): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(416, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(416, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer7): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(448, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer8): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(480, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer9): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer10): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(544, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(544, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer11): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(576, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer12): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(608, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(608, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer13): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(640, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer14): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(672, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer15): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(704, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(704, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer16): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(736, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(736, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer17): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer18): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(800, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer19): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(832, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer20): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(864, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(864, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer21): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer22): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(928, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(928, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer23): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(960, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer24): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(992, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(992, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (transition3): _Transition(\n",
       "      (norm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "    )\n",
       "    (denseblock4): _DenseBlock(\n",
       "      (denselayer1): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer2): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(544, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(544, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer3): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(576, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer4): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(608, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(608, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer5): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(640, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer6): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(672, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer7): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(704, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(704, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer8): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(736, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(736, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer9): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer10): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(800, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer11): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(832, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer12): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(864, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(864, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer13): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer14): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(928, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(928, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer15): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(960, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer16): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(992, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (conv1): Conv2d(992, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (norm5): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (classifier): Linear(in_features=1024, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = models.densenet121(weights = models.DenseNet121_Weights.DEFAULT)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model is built out of two main parts, the features and the classifier. The features part is a stack of convolutional layers and overall works as a feature detector that can be fed into a classifier. The classifier part is a single fully-connected layer `(classifier): Linear(in_features=1024, out_features=1000)`. This layer was trained on the ImageNet dataset, so it won't work for our specific problem. That means we need to replace the classifier, but the features will work perfectly on their own. In general, I think about pre-trained networks as amazingly good feature detectors that can be used as the input for simple feed-forward classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze parameters so we don't backprop through them\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "from collections import OrderedDict\n",
    "classifier = nn.Sequential(OrderedDict([\n",
    "                          ('fc1', nn.Linear(1024, 500)),\n",
    "                          ('relu', nn.ReLU()),\n",
    "                          ('fc2', nn.Linear(500, 2)),\n",
    "                          ('output', nn.LogSoftmax(dim=1))\n",
    "                          ]))\n",
    "    \n",
    "model.classifier = classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our model built, we need to train the classifier. However, now we're using a **really deep** neural network. If you try to train this on a CPU like normal, it will take a long, long time. Instead, we're going to use the GPU to do the calculations. The linear algebra computations are done in parallel on the GPU leading to 100x increased training speeds. It's also possible to train on multiple GPUs, further decreasing training time.\n",
    "\n",
    "PyTorch, along with pretty much every other deep learning framework, uses [CUDA](https://developer.nvidia.com/cuda-zone) to efficiently compute the forward and backwards passes on the GPU. In PyTorch, you move your model parameters and other tensors to the GPU memory using `model.to('cuda')`. You can move them back from the GPU with `model.to('cpu')` which you'll commonly do when you need to operate on the network output outside of PyTorch. As a demonstration of the increased speed, I'll compare how long it takes to perform a forward and backward pass with and without a GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device = cpu; Time per batch: 1.300 seconds\n",
      "Device = cuda; Time per batch: 0.009 seconds\n"
     ]
    }
   ],
   "source": [
    "for device in ['cpu', 'cuda']:\n",
    "\n",
    "    criterion = nn.NLLLoss()\n",
    "    # Only train the classifier parameters, feature parameters are frozen\n",
    "    optimizer = optim.Adam(model.classifier.parameters(), lr=0.001)\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    for ii, (inputs, labels) in enumerate(trainloader):\n",
    "\n",
    "        # Move input and label tensors to the GPU\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        start = time.time()\n",
    "\n",
    "        outputs = model.forward(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if ii==3:\n",
    "            break\n",
    "        \n",
    "    print(f\"Device = {device}; Time per batch: {(time.time() - start)/3:.3f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can write device agnostic code which will automatically use CUDA if it's enabled like so:\n",
    "```python\n",
    "# at beginning of the script\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "...\n",
    "\n",
    "# then whenever you get a new Tensor or Module\n",
    "# this won't copy if they are already on the desired device\n",
    "input = data.to(device)\n",
    "model = MyModule(...).to(device)\n",
    "```\n",
    "\n",
    "From here, I'll let you finish training the model. The process is the same as before except now your model is much more powerful. You should get better than 95% accuracy easily.\n",
    "\n",
    ">**Exercise:** Train a pretrained models to classify the cat and dog images. Continue with the DenseNet model, or try ResNet, it's also a good model to try out first. Make sure you are only training the classifier and the parameters for the features part are frozen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.densenet121(weights = models.DenseNet121_Weights.DEFAULT)\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "classifier = nn.Sequential(OrderedDict(\n",
    "            [('fc1', nn.Linear(1024, 256)),\n",
    "             ('relu1', nn.ReLU()),\n",
    "             ('dropout1', nn.Dropout(p = 0.2)), \n",
    "             ('fc2', nn.Linear(256, 2)),\n",
    "             ('output', nn.LogSoftmax(dim = 1))\n",
    "            ]))\n",
    "\n",
    "model.classifier = classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device cuda\n",
      "The 1-th epochs\n",
      "  running loss: 1.7297033593058586\n",
      "  test_accuracy: 78.3203125 %\n",
      "The 1-th epochs\n",
      "  running loss: 3.898295544087887\n",
      "  test_accuracy: 85.50781011581421 %\n",
      "The 1-th epochs\n",
      "  running loss: 5.450270138680935\n",
      "  test_accuracy: 93.1640625 %\n",
      "The 1-th epochs\n",
      "  running loss: 6.64800775796175\n",
      "  test_accuracy: 96.875 %\n",
      "The 1-th epochs\n",
      "  running loss: 7.681717284023762\n",
      "  test_accuracy: 96.1718738079071 %\n",
      "The 1-th epochs\n",
      "  running loss: 8.653387695550919\n",
      "  test_accuracy: 97.1484363079071 %\n",
      "The 1-th epochs\n",
      "  running loss: 9.39971423894167\n",
      "  test_accuracy: 97.3828136920929 %\n",
      "The 1-th epochs\n",
      "  running loss: 10.034282006323338\n",
      "  test_accuracy: 96.2109386920929 %\n",
      "The 1-th epochs\n",
      "  running loss: 10.956619121134281\n",
      "  test_accuracy: 97.42187261581421 %\n",
      "The 1-th epochs\n",
      "  running loss: 11.951220236718655\n",
      "  test_accuracy: 96.64062261581421 %\n",
      "The 1-th epochs\n",
      "  running loss: 12.658864222466946\n",
      "  test_accuracy: 97.30468988418579 %\n",
      "The 1-th epochs\n",
      "  running loss: 13.32079042494297\n",
      "  test_accuracy: 96.64062261581421 %\n",
      "The 1-th epochs\n",
      "  running loss: 14.100856855511665\n",
      "  test_accuracy: 96.24999761581421 %\n",
      "The 1-th epochs\n",
      "  running loss: 14.820692129433155\n",
      "  test_accuracy: 97.30468988418579 %\n",
      "The 1-th epochs\n",
      "  running loss: 15.669808343052864\n",
      "  test_accuracy: 96.6015636920929 %\n",
      "The 1-th epochs\n",
      "  running loss: 16.59272427856922\n",
      "  test_accuracy: 97.50000238418579 %\n",
      "The 1-th epochs\n",
      "  running loss: 17.21038705855608\n",
      "  test_accuracy: 97.4609375 %\n",
      "The 1-th epochs\n",
      "  running loss: 17.980137884616852\n",
      "  test_accuracy: 97.30468988418579 %\n",
      "The 1-th epochs\n",
      "  running loss: 18.69202444702387\n",
      "  test_accuracy: 96.9921886920929 %\n",
      "The 1-th epochs\n",
      "  running loss: 19.405045494437218\n",
      "  test_accuracy: 96.875 %\n",
      "The 1-th epochs\n",
      "  running loss: 20.252441972494125\n",
      "  test_accuracy: 97.30468988418579 %\n",
      "The 1-th epochs\n",
      "  running loss: 21.269882306456566\n",
      "  test_accuracy: 95.9765613079071 %\n",
      "The 1-th epochs\n",
      "  running loss: 21.82093319296837\n",
      "  test_accuracy: 97.0703125 %\n",
      "The 1-th epochs\n",
      "  running loss: 22.567018292844296\n",
      "  test_accuracy: 96.05468511581421 %\n",
      "The 1-th epochs\n",
      "  running loss: 23.294959470629692\n",
      "  test_accuracy: 96.05468511581421 %\n",
      "The 1-th epochs\n",
      "  running loss: 24.2081526145339\n",
      "  test_accuracy: 97.10937738418579 %\n",
      "The 1-th epochs\n",
      "  running loss: 25.275061406195164\n",
      "  test_accuracy: 97.3828136920929 %\n",
      "The 1-th epochs\n",
      "  running loss: 25.945999816060066\n",
      "  test_accuracy: 97.3437488079071 %\n",
      "The 1-th epochs\n",
      "  running loss: 26.61101634055376\n",
      "  test_accuracy: 96.2890625 %\n",
      "The 1-th epochs\n",
      "  running loss: 27.36420740187168\n",
      "  test_accuracy: 97.22656011581421 %\n",
      "The 1-th epochs\n",
      "  running loss: 28.195633217692375\n",
      "  test_accuracy: 97.22656011581421 %\n",
      "The 1-th epochs\n",
      "  running loss: 28.948070347309113\n",
      "  test_accuracy: 97.3828136920929 %\n",
      "The 1-th epochs\n",
      "  running loss: 29.756267219781876\n",
      "  test_accuracy: 97.1484363079071 %\n",
      "The 1-th epochs\n",
      "  running loss: 30.48920377343893\n",
      "  test_accuracy: 97.22656011581421 %\n",
      "The 1-th epochs\n",
      "  running loss: 31.37191990762949\n",
      "  test_accuracy: 97.42187261581421 %\n",
      "The 1-th epochs\n",
      "  running loss: 31.93879346549511\n",
      "  test_accuracy: 97.265625 %\n",
      "The 1-th epochs\n",
      "  running loss: 32.58881142735481\n",
      "  test_accuracy: 97.3437488079071 %\n",
      "The 1-th epochs\n",
      "  running loss: 33.285543508827686\n",
      "  test_accuracy: 97.1484363079071 %\n",
      "The 1-th epochs\n",
      "  running loss: 33.916000090539455\n",
      "  test_accuracy: 97.265625 %\n",
      "The 1-th epochs\n",
      "  running loss: 34.63876035064459\n",
      "  test_accuracy: 97.10937738418579 %\n",
      "The 1-th epochs\n",
      "  running loss: 35.23788844794035\n",
      "  test_accuracy: 97.265625 %\n",
      "The 1-th epochs\n",
      "  running loss: 35.942212991416454\n",
      "  test_accuracy: 97.03124761581421 %\n",
      "The 1-th epochs\n",
      "  running loss: 36.634531043469906\n",
      "  test_accuracy: 97.03124761581421 %\n",
      "The 1-th epochs\n",
      "  running loss: 37.286142855882645\n",
      "  test_accuracy: 97.22656011581421 %\n",
      "The 1-th epochs\n",
      "  running loss: 37.91815933585167\n",
      "  test_accuracy: 97.265625 %\n",
      "The 1-th epochs\n",
      "  running loss: 38.60922134667635\n",
      "  test_accuracy: 97.3828136920929 %\n",
      "The 1-th epochs\n",
      "  running loss: 39.33612251281738\n",
      "  test_accuracy: 97.3828136920929 %\n",
      "The 1-th epochs\n",
      "  running loss: 40.23471865057945\n",
      "  test_accuracy: 97.22656011581421 %\n",
      "The 1-th epochs\n",
      "  running loss: 40.97838746383786\n",
      "  test_accuracy: 97.30468988418579 %\n",
      "The 1-th epochs\n",
      "  running loss: 41.64126784726977\n",
      "  test_accuracy: 97.4609375 %\n",
      "The 1-th epochs\n",
      "  running loss: 42.27761264145374\n",
      "  test_accuracy: 97.30468988418579 %\n",
      "The 1-th epochs\n",
      "  running loss: 43.09364803135395\n",
      "  test_accuracy: 97.42187261581421 %\n",
      "The 1-th epochs\n",
      "  running loss: 43.86382335424423\n",
      "  test_accuracy: 97.30468988418579 %\n",
      "The 1-th epochs\n",
      "  running loss: 44.59992138296366\n",
      "  test_accuracy: 97.1875011920929 %\n",
      "The 1-th epochs\n",
      "  running loss: 45.312742955982685\n",
      "  test_accuracy: 97.3828136920929 %\n",
      "The 1-th epochs\n",
      "  running loss: 46.007792703807354\n",
      "  test_accuracy: 97.265625 %\n",
      "The 1-th epochs\n",
      "  running loss: 46.596264623105526\n",
      "  test_accuracy: 97.0703125 %\n",
      "The 1-th epochs\n",
      "  running loss: 47.170410353690386\n",
      "  test_accuracy: 97.30468988418579 %\n",
      "The 1-th epochs\n",
      "  running loss: 47.90879426524043\n",
      "  test_accuracy: 97.3437488079071 %\n",
      "The 1-th epochs\n",
      "  running loss: 48.75366723164916\n",
      "  test_accuracy: 97.42187261581421 %\n",
      "The 1-th epochs\n",
      "  running loss: 49.40999935939908\n",
      "  test_accuracy: 97.4609375 %\n",
      "The 1-th epochs\n",
      "  running loss: 50.13708275556564\n",
      "  test_accuracy: 97.5781261920929 %\n",
      "The 1-th epochs\n",
      "  running loss: 50.8915965333581\n",
      "  test_accuracy: 97.4609375 %\n",
      "The 1-th epochs\n",
      "  running loss: 51.42883063852787\n",
      "  test_accuracy: 97.42187261581421 %\n",
      "The 1-th epochs\n",
      "  running loss: 52.02536978945136\n",
      "  test_accuracy: 97.4609375 %\n",
      "The 1-th epochs\n",
      "  running loss: 52.847395706921816\n",
      "  test_accuracy: 97.4609375 %\n",
      "The 1-th epochs\n",
      "  running loss: 53.44016721472144\n",
      "  test_accuracy: 96.7968761920929 %\n",
      "The 1-th epochs\n",
      "  running loss: 54.00095608457923\n",
      "  test_accuracy: 97.4609375 %\n",
      "The 1-th epochs\n",
      "  running loss: 54.8065022341907\n",
      "  test_accuracy: 97.42187261581421 %\n",
      "The 1-th epochs\n",
      "  running loss: 55.57418878003955\n",
      "  test_accuracy: 97.3437488079071 %\n",
      "test loss [1.247358798980713, 1.3616396188735962, 1.1398227214813232, 1.1478679180145264, 1.257014274597168, 1.1786772012710571, 0.8626853227615356, 1.098385214805603, 1.0996419191360474, 1.3025460243225098, 0.9895862340927124, 1.0852744579315186, 0.9195919632911682, 1.0494651794433594, 1.0379427671432495, 1.0113565921783447, 1.1429567337036133, 1.0993294715881348, 1.0562878847122192, 0.7230890393257141, 0.003467052010819316, 0.005193050485104322, 0.0018866630271077156, 0.007370081264525652, 0.0019252882339060307, 0.0035316525027155876, 0.0035298592410981655, 0.0006610886193811893, 0.0017663892358541489, 0.005794265773147345, 0.0027616710867732763, 0.0028578010387718678, 0.0022675818763673306, 0.005425355397164822, 0.003887442173436284, 0.0016628169687464833, 0.0015721742529422045, 0.001224967883899808, 0.001973990583792329, 0.0003899814037140459, 0.009216846898198128, 0.020885279402136803, 0.011418788693845272, 0.006419690325856209, 0.015426453202962875, 0.009770049713551998, 0.007247642148286104, 0.014004542492330074, 0.008947528898715973, 0.013115640729665756, 0.0053166914731264114, 0.0108041325584054, 0.005527604836970568, 0.007938332855701447, 0.006390264257788658, 0.012518025003373623, 0.006806587800383568, 0.006791090592741966, 0.011632484383881092, 0.26816219091415405, 0.8174443244934082, 0.6851395964622498, 0.599234402179718, 0.6173180937767029, 0.6626888513565063, 0.6975724101066589, 0.6600668430328369, 0.35545122623443604, 0.4950062036514282, 0.7673875093460083, 0.5432207584381104, 0.7182487845420837, 0.531274139881134, 0.7599242329597473, 0.7624512314796448, 0.5750613212585449, 0.5100075006484985, 0.623223602771759, 0.5586992502212524, 0.36078646779060364, 0.33230963349342346, 0.3970668613910675, 0.3208959400653839, 0.2739766538143158, 0.33387795090675354, 0.29946234822273254, 0.22921770811080933, 0.3119592070579529, 0.29407984018325806, 0.35616347193717957, 0.22842654585838318, 0.2869924008846283, 0.2167527675628662, 0.26859205961227417, 0.25365331768989563, 0.2770825922489166, 0.2878580093383789, 0.2578589916229248, 0.2729429602622986, 0.1991095244884491, 0.06442081928253174, 0.06357047706842422, 0.03903627023100853, 0.0514056570827961, 0.044194545596838, 0.05974315106868744, 0.05370308458805084, 0.020678827539086342, 0.038376715034246445, 0.06783989071846008, 0.044479139149188995, 0.05569341778755188, 0.03976821154356003, 0.07014816254377365, 0.06494338065385818, 0.038881830871105194, 0.03544997796416283, 0.03807714208960533, 0.04085405543446541, 0.01883840188384056, 0.05871795862913132, 0.11711271852254868, 0.07150845229625702, 0.04170778766274452, 0.08280882984399796, 0.05566342920064926, 0.04788699373602867, 0.08051043003797531, 0.05637180060148239, 0.08945067971944809, 0.03502923995256424, 0.06164855137467384, 0.03264102712273598, 0.053094517439603806, 0.03751714527606964, 0.06691069155931473, 0.04399858042597771, 0.041941557079553604, 0.0695331022143364, 0.09181743860244751, 0.2400072067975998, 0.19310374557971954, 0.11705125123262405, 0.15019623935222626, 0.13894832134246826, 0.20614567399024963, 0.15501965582370758, 0.051096007227897644, 0.10116425901651382, 0.20387305319309235, 0.12622952461242676, 0.18631313741207123, 0.1101161316037178, 0.2290455549955368, 0.20960314571857452, 0.11775985360145569, 0.09245923161506653, 0.12841388583183289, 0.11603567749261856, 0.047130197286605835, 0.16176527738571167, 0.2724624574184418, 0.1812511533498764, 0.11625289171934128, 0.1939486414194107, 0.14120899140834808, 0.12142563611268997, 0.19056393206119537, 0.1456848829984665, 0.21972611546516418, 0.09549199789762497, 0.14796040952205658, 0.08870894461870193, 0.1349378526210785, 0.10179521143436432, 0.16188742220401764, 0.13285478949546814, 0.11294591426849365, 0.15281382203102112, 0.09795645624399185, 0.08975043147802353, 0.08960048109292984, 0.028274085372686386, 0.056661028414964676, 0.035274751484394073, 0.07468359917402267, 0.04672999680042267, 0.009800737723708153, 0.026452144607901573, 0.07196886837482452, 0.038083866238594055, 0.060151323676109314, 0.03133616968989372, 0.09272882342338562, 0.06926792114973068, 0.028485679998993874, 0.021399375051259995, 0.03169351443648338, 0.030864307656884193, 0.008510910905897617, 0.07530798017978668, 0.1912483423948288, 0.09886009246110916, 0.05331702530384064, 0.1229364201426506, 0.06909610331058502, 0.0700465589761734, 0.12010152637958527, 0.07558096200227737, 0.14026111364364624, 0.041064050048589706, 0.08238989859819412, 0.038313306868076324, 0.07365865260362625, 0.04297656938433647, 0.09555775672197342, 0.05979035049676895, 0.05322922021150589, 0.10270404815673828, 0.06432095915079117, 0.15296533703804016, 0.13593681156635284, 0.037513598799705505, 0.07983513176441193, 0.051756713539361954, 0.12401019781827927, 0.06495343148708344, 0.010605287738144398, 0.03315379098057747, 0.10592418909072876, 0.053240325301885605, 0.09733444452285767, 0.04303968697786331, 0.1493256390094757, 0.10716865956783295, 0.04084969684481621, 0.024956507608294487, 0.04955451935529709, 0.04458995908498764, 0.009599898010492325, 0.04612461477518082, 0.1608944535255432, 0.06715290248394012, 0.03321574255824089, 0.10058337450027466, 0.04500297084450722, 0.05102987587451935, 0.09577188640832901, 0.05028299242258072, 0.10898436605930328, 0.024722790345549583, 0.05961492285132408, 0.022056445479393005, 0.05399426817893982, 0.026135770604014397, 0.0703134536743164, 0.03675513342022896, 0.0316900834441185, 0.08641105890274048, 0.056199125945568085, 0.1990489661693573, 0.16784343123435974, 0.04261261224746704, 0.09637567400932312, 0.06014205142855644, 0.1632993519306183, 0.07492253929376602, 0.010043730027973652, 0.03534257039427757, 0.12698818743228912, 0.0629575252532959, 0.12414193153381348, 0.05019723251461983, 0.18987509608268738, 0.13311567902565002, 0.05118764564394951, 0.02505926415324211, 0.061821069568395615, 0.05412260442972183, 0.009428855031728745, 0.1488358974456787, 0.3283630609512329, 0.18170762062072754, 0.10857132822275162, 0.20204496383666992, 0.12091180682182312, 0.12892724573612213, 0.20704349875450134, 0.13542532920837402, 0.25113335251808167, 0.0806761309504509, 0.1486799269914627, 0.07233130186796188, 0.1294039487838745, 0.0873735249042511, 0.16264399886131287, 0.12790124118328094, 0.09521660953760147, 0.17250967025756836, 0.0710260197520256, 0.08367742598056793, 0.0943329855799675, 0.009818698279559612, 0.047173358500003815, 0.01383642852306366, 0.0649752989411354, 0.02219289168715477, 0.0019232961349189281, 0.008757836185395718, 0.047630347311496735, 0.021617218852043152, 0.0393742099404335, 0.01761445961892605, 0.08428045362234116, 0.0438992865383625, 0.01279610488563776, 0.00554724782705307, 0.01730860210955143, 0.016404271125793457, 0.001798264100216329, 0.04410199075937271, 0.1787300705909729, 0.06616123765707016, 0.03133174777030945, 0.10854404419660568, 0.044219017028808594, 0.053714361041784286, 0.10407382994890213, 0.05072920024394989, 0.12354093790054321, 0.023273883387446404, 0.061319589614868164, 0.019130654633045197, 0.054245829582214355, 0.026695655658841133, 0.07500442117452621, 0.03645986318588257, 0.02788674272596836, 0.09805319458246231, 0.0502665676176548, 0.20276229083538055, 0.16239307820796967, 0.03250465542078018, 0.09079299867153168, 0.044932570308446884, 0.16183125972747803, 0.05877356603741646, 0.004953588359057903, 0.024502655491232872, 0.11297663301229477, 0.05573223903775215, 0.11003869771957397, 0.04154636338353157, 0.18821842968463898, 0.11402733623981476, 0.04637804254889488, 0.014989024959504604, 0.050046682357788086, 0.04377906769514084, 0.004545004107058048, 0.1008923351764679, 0.2766416072845459, 0.13335786759853363, 0.07576602697372437, 0.16548480093479156, 0.08965331315994263, 0.10492882877588272, 0.17032989859580994, 0.1058281660079956, 0.21943578124046326, 0.0530046671628952, 0.11059469729661942, 0.04635956138372421, 0.09829302877187729, 0.06118442863225937, 0.12703774869441986, 0.09017395973205566, 0.06290145963430405, 0.15675584971904755, 0.05448664724826813, 0.11748865246772766, 0.10871642827987671, 0.012470339424908161, 0.0515584871172905, 0.01765464060008526, 0.08531378954648972, 0.027372833341360092, 0.0019314701203256845, 0.010484816506505013, 0.06289392709732056, 0.030469970777630806, 0.05185532942414284, 0.022579791024327278, 0.11329460144042969, 0.0540706031024456, 0.021225299686193466, 0.006726591382175684, 0.023280490189790726, 0.0207756869494915, 0.0019088415428996086, 0.022716231644153595, 0.12750020623207092, 0.03738163411617279, 0.01745559275150299, 0.08189977705478668, 0.030481383204460144, 0.03678903728723526, 0.0738195925951004, 0.03477694094181061, 0.09158262610435486, 0.011705739423632622, 0.03829485923051834, 0.010156946256756783, 0.03741675242781639, 0.014003416523337364, 0.05064311623573303, 0.019681910052895546, 0.013621864840388298, 0.08069391548633575, 0.05660266429185867, 0.2699243724346161, 0.19090166687965393, 0.05126156657934189, 0.11602567881345749, 0.06769106537103653, 0.20668870210647583, 0.08748090267181396, 0.007878945209085941, 0.03581009805202484, 0.1580580770969391, 0.07939416170120239, 0.15475262701511383, 0.05527075007557869, 0.2552669644355774, 0.1507261097431183, 0.08027665317058563, 0.026286907494068146, 0.07910770922899246, 0.06241264566779137, 0.007363481447100639, 0.1325497031211853, 0.3111855685710907, 0.15725719928741455, 0.09763786196708679, 0.18765394389629364, 0.11811521649360657, 0.13061131536960602, 0.19976648688316345, 0.13963346183300018, 0.25742363929748535, 0.06436614692211151, 0.12501275539398193, 0.05848895013332367, 0.1148439571261406, 0.07396424561738968, 0.15151971578598022, 0.11129998415708542, 0.07432293146848679, 0.18739648163318634, 0.062678262591362, 0.08866757154464722, 0.08882256597280502, 0.008286530151963234, 0.03728107735514641, 0.010785678401589394, 0.05747875198721886, 0.018999505788087845, 0.0014114204095676541, 0.006465043406933546, 0.05097992345690727, 0.02495657466351986, 0.03591247648000717, 0.015754126012325287, 0.09281732141971588, 0.03733672574162483, 0.01683885231614113, 0.00513929408043623, 0.019991623237729073, 0.014521402306854725, 0.0011891095200553536, 0.008962132036685944, 0.06777048110961914, 0.015172885730862617, 0.006203185301274061, 0.056733839213848114, 0.01801464706659317, 0.015518564730882645, 0.042433962225914, 0.016124587506055832, 0.03871716558933258, 0.004171221982687712, 0.017724568024277687, 0.003985042218118906, 0.01599946618080139, 0.004674447234719992, 0.02849668636918068, 0.00631884066388011, 0.005291949026286602, 0.05026400461792946, 0.08106653392314911, 0.38864192366600037, 0.26929333806037903, 0.10886504501104355, 0.18363046646118164, 0.12367455661296844, 0.3000166416168213, 0.1521902084350586, 0.020141534507274628, 0.06311837583780289, 0.2634284496307373, 0.13306814432144165, 0.24730059504508972, 0.08856958150863647, 0.37292155623435974, 0.25138792395591736, 0.1502067595720291, 0.05811502784490585, 0.15102843940258026, 0.11476349085569382, 0.013554949313402176, 0.06213216483592987, 0.20345114171504974, 0.08015163242816925, 0.04321622848510742, 0.12591490149497986, 0.06871957331895828, 0.06780941039323807, 0.12601663172245026, 0.07250981032848358, 0.14618979394435883, 0.027391452342271805, 0.0664534866809845, 0.024532753974199295, 0.05779172107577324, 0.0311061292886734, 0.09493405371904373, 0.04337431862950325, 0.03315449133515358, 0.13532626628875732, 0.04899929463863373, 0.15954077243804932, 0.13714538514614105, 0.02183271385729313, 0.06207151338458061, 0.022449593991041183, 0.10514435917139053, 0.0385601781308651, 0.0034823091700673103, 0.012190110050141811, 0.09287042915821075, 0.04518928751349449, 0.07208370417356491, 0.025804702192544937, 0.15664124488830566, 0.07715662568807602, 0.04276304692029953, 0.010796373710036278, 0.04549846425652504, 0.031226234510540962, 0.0022869184613227844, 0.11283822357654572, 0.277870774269104, 0.13328751921653748, 0.0768551453948021, 0.1741810441017151, 0.1048450618982315, 0.10221622884273529, 0.1751832365989685, 0.1157001480460167, 0.21055401861667633, 0.05130257084965706, 0.10384322702884674, 0.04203426465392113, 0.08717206120491028, 0.05828557163476944, 0.1380791962146759, 0.08167947083711624, 0.05668109282851219, 0.1864117980003357, 0.05759806931018829, 0.10396470874547958, 0.10682524740695953, 0.011624922975897789, 0.04145856574177742, 0.011031490750610828, 0.06520171463489532, 0.021825799718499184, 0.0018117306753993034, 0.005949437618255615, 0.06104479357600212, 0.03041958622634411, 0.042135559022426605, 0.01623271033167839, 0.1100083738565445, 0.04774962365627289, 0.02277599461376667, 0.005573376081883907, 0.02613903395831585, 0.017796937376260757, 0.0011277524754405022, 0.035019028931856155, 0.14647896587848663, 0.045916251838207245, 0.022415243089199066, 0.10557723045349121, 0.04420584812760353, 0.03999125212430954, 0.08722713589668274, 0.04179360345005989, 0.08947262167930603, 0.01338214986026287, 0.04439448565244675, 0.011636157520115376, 0.032443031668663025, 0.01702408865094185, 0.06363789737224579, 0.023734966292977333, 0.01607866771519184, 0.11592858284711838, 0.05752020329236984, 0.2233879566192627, 0.17598578333854675, 0.04216323792934418, 0.07808875292539597, 0.03962203860282898, 0.1506907194852829, 0.06138626113533974, 0.006541447248309851, 0.018988382071256638, 0.13140667974948883, 0.07084895670413971, 0.11691474169492722, 0.03900613263249397, 0.22188858687877655, 0.1276833713054657, 0.06805889308452606, 0.0190600473433733, 0.06583349406719208, 0.05167967081069946, 0.003710811259225011, 0.036911096423864365, 0.15039093792438507, 0.0474427305161953, 0.025547167286276817, 0.10967285931110382, 0.043885745108127594, 0.041419483721256256, 0.08595575392246246, 0.044772833585739136, 0.08921816200017929, 0.013039924204349518, 0.04614833742380142, 0.012112822383642197, 0.03308413550257683, 0.017868012189865112, 0.06417173892259598, 0.025858616456389427, 0.016820387914776802, 0.12222094088792801, 0.058454107493162155, 0.21471810340881348, 0.17471297085285187, 0.04041679576039314, 0.07383864372968674, 0.03761591017246246, 0.14235685765743256, 0.0569845512509346, 0.006238491740077734, 0.01789686642587185, 0.12183703482151031, 0.0684773400425911, 0.11019904166460037, 0.03797357529401779, 0.21261189877986908, 0.11847174167633057, 0.06765493005514145, 0.019268300384283066, 0.06239960342645645, 0.04892147332429886, 0.004196393769234419, 0.072309710085392, 0.21641115844249725, 0.08488950878381729, 0.05407794192433357, 0.1444069743156433, 0.06840535998344421, 0.06922563910484314, 0.12371920794248581, 0.07809875905513763, 0.1354674994945526, 0.025140555575489998, 0.07516467571258545, 0.024104824289679527, 0.05414246767759323, 0.0363471582531929, 0.09641090780496597, 0.054747626185417175, 0.033427637070417404, 0.1624411791563034, 0.057939834892749786, 0.13912154734134674, 0.13319185376167297, 0.02159770391881466, 0.0496651828289032, 0.019352976232767105, 0.08659446984529495, 0.030747530981898308, 0.003109370358288288, 0.009285304695367813, 0.07353316992521286, 0.044436242431402206, 0.06508592516183853, 0.02514629065990448, 0.14526163041591644, 0.06831958144903183, 0.03772026672959328, 0.010291852056980133, 0.03761247918009758, 0.028032196685671806, 0.0024011710193008184, 0.0948854386806488, 0.25057166814804077, 0.10942191630601883, 0.07480048388242722, 0.16376210749149323, 0.08000876754522324, 0.08793172240257263, 0.14386333525180817, 0.09818729758262634, 0.15880510210990906, 0.034568462520837784, 0.09426294267177582, 0.030199630185961723, 0.06816401332616806, 0.04882331192493439, 0.11749865859746933, 0.07310409098863602, 0.04479443281888962, 0.18205401301383972, 0.06093016639351845, 0.11280513554811478, 0.11741901934146881, 0.016497908160090446, 0.04450349509716034, 0.013722347095608711, 0.06709365546703339, 0.022452477365732193, 0.0019533156882971525, 0.0069157034158706665, 0.056418538093566895, 0.03426458314061165, 0.050138287246227264, 0.020738329738378525, 0.12116953730583191, 0.050323180854320526, 0.0285180751234293, 0.007193659897893667, 0.02997557632625103, 0.020700450986623764, 0.0017884477274492383, 0.10329365730285645, 0.26798146963119507, 0.12040065973997116, 0.08723768591880798, 0.17294541001319885, 0.08126083016395569, 0.09591688215732574, 0.15024465322494507, 0.1044311448931694, 0.16627272963523865, 0.039697617292404175, 0.10297646373510361, 0.02976221963763237, 0.07576590776443481, 0.05457187443971634, 0.12466958910226822, 0.08267930150032043, 0.047696303576231, 0.19044940173625946, 0.05966461822390556, 0.1064939945936203, 0.11175528168678284, 0.01585371606051922, 0.04457009956240654, 0.012086228467524052, 0.06022530049085617, 0.02046673186123371, 0.0014382284134626389, 0.006320759654045105, 0.051673177629709244, 0.029403679072856903, 0.04529305174946785, 0.01990339532494545, 0.11752671003341675, 0.041760873049497604, 0.027743428945541382, 0.005857216194272041, 0.025915686041116714, 0.01766481250524521, 0.0015720500377938151, 0.022091424092650414, 0.11963021010160446, 0.03291338309645653, 0.020789388567209244, 0.09626130014657974, 0.025272604078054428, 0.034161992371082306, 0.0603204146027565, 0.03055795282125473, 0.054382726550102234, 0.008616928942501545, 0.03747740015387535, 0.005086912773549557, 0.02747744508087635, 0.011850529350340366, 0.051454007625579834, 0.015469267033040524, 0.009856343269348145, 0.0986640602350235, 0.06404359638690948, 0.2716269791126251, 0.2041039913892746, 0.07023755460977554, 0.11047708988189697, 0.06206155940890312, 0.17472364008426666, 0.0791497528553009, 0.006498323753476143, 0.028820930048823357, 0.1416902244091034, 0.07647580653429031, 0.14939472079277039, 0.053296659141778946, 0.26818281412124634, 0.13688498735427856, 0.1042894795536995, 0.02391543611884117, 0.08643811196088791, 0.058734308928251266, 0.006344207096844912, 0.21015582978725433, 0.38510772585868835, 0.2439478188753128, 0.1712329089641571, 0.2609679698944092, 0.1600213199853897, 0.16561740636825562, 0.24774430692195892, 0.19782277941703796, 0.27765941619873047, 0.09835533052682877, 0.19350773096084595, 0.07150150090456009, 0.15012489259243011, 0.11878623068332672, 0.1999175101518631, 0.17971090972423553, 0.11615899950265884, 0.25214406847953796, 0.09274028241634369, 0.05538419634103775, 0.06400450319051743, 0.008071069605648518, 0.02534840628504753, 0.005872499197721481, 0.021798044443130493, 0.0095311738550663, 0.0007252919604070485, 0.0032782121561467648, 0.02830643020570278, 0.012137635610997677, 0.016489289700984955, 0.01082690991461277, 0.058417655527591705, 0.015539923682808876, 0.013167330995202065, 0.0026898211799561977, 0.010774753987789154, 0.006694579031318426, 0.0007868316606618464, 0.017857374623417854, 0.09798813611268997, 0.02775469422340393, 0.019416922703385353, 0.0863301008939743, 0.023973869159817696, 0.02744276449084282, 0.053527429699897766, 0.02683258429169655, 0.045514948666095734, 0.007167283445596695, 0.03627866506576538, 0.004476988222450018, 0.025347046554088593, 0.010337922722101212, 0.042814720422029495, 0.012647041119635105, 0.009422731585800648, 0.08193803578615189, 0.06553085148334503, 0.2958630323410034, 0.21724362671375275, 0.09038788080215454, 0.12345879524946213, 0.07954245805740356, 0.18432308733463287, 0.09122002869844437, 0.010035295970737934, 0.03574983775615692, 0.16182945668697357, 0.0810764953494072, 0.16338473558425903, 0.06416084617376328, 0.2882314622402191, 0.15591838955879211, 0.13057389855384827, 0.030282223597168922, 0.11375013738870621, 0.06903335452079773, 0.007831764407455921, 0.19239221513271332, 0.36293745040893555, 0.22787733376026154, 0.16691164672374725, 0.2453068196773529, 0.16019956767559052, 0.16089533269405365, 0.24424946308135986, 0.18940329551696777, 0.2639125883579254, 0.085629403591156, 0.1837599128484726, 0.06866846978664398, 0.14459773898124695, 0.10991448163986206, 0.18913324177265167, 0.16196852922439575, 0.11349765211343765, 0.23707124590873718, 0.08569548279047012, 0.05800273269414902, 0.071872778236866, 0.008171872235834599, 0.024362115189433098, 0.006438068114221096, 0.02105390466749668, 0.009237093850970268, 0.0008766577811911702, 0.003447336843237281, 0.028987817466259003, 0.011747031472623348, 0.016913587227463722, 0.011899878270924091, 0.05896157771348953, 0.01785939559340477, 0.015429586172103882, 0.0029442990198731422, 0.0144464997574687, 0.007695725653320551, 0.0008648386574350297, 0.006631753407418728, 0.059015657752752304, 0.012852197512984276, 0.00853737909346819, 0.06896807253360748, 0.01539610419422388, 0.013459564186632633, 0.03628630191087723, 0.012732184492051601, 0.025419041514396667, 0.0026601424906402826, 0.02171260304749012, 0.0020917444489896297, 0.013519655913114548, 0.004461153410375118, 0.024542856961488724, 0.0045829094015061855, 0.0044868215918540955, 0.050293825566768646, 0.08617135137319565, 0.4200814962387085, 0.2968330681324005, 0.1421053558588028, 0.20113076269626617, 0.1409432291984558, 0.2756529748439789, 0.1472974419593811, 0.02023991569876671, 0.06256017833948135, 0.23856952786445618, 0.12863892316818237, 0.258599191904068, 0.10431444644927979, 0.4007604420185089, 0.2489338368177414, 0.19698277115821838, 0.05497443675994873, 0.18229275941848755, 0.11541815847158432, 0.014716449193656445, 0.08995305746793747, 0.2495097666978836, 0.11842242628335953, 0.08732711523771286, 0.1638139933347702, 0.09613928943872452, 0.09313813596963882, 0.15576879680156708, 0.10592259466648102, 0.17357848584651947, 0.04004710912704468, 0.10987085103988647, 0.029512474313378334, 0.0841774046421051, 0.05316263437271118, 0.12305637449026108, 0.07390038669109344, 0.054895978420972824, 0.1749923676252365, 0.047332871705293655, 0.11866134405136108, 0.11457368731498718, 0.020290931686758995, 0.03969119116663933, 0.01506483182311058, 0.04672650247812271, 0.02142394706606865, 0.0017326107481494546, 0.007382839452475309, 0.06376851350069046, 0.0270079318434, 0.042821004986763, 0.024558616802096367, 0.11811939626932144, 0.044050171971321106, 0.03745086118578911, 0.005657926667481661, 0.03087422624230385, 0.018768001347780228, 0.0015790262259542942, 0.04965369030833244, 0.16494084894657135, 0.06639812141656876, 0.048341304063797, 0.11561960726976395, 0.06002426892518997, 0.05486992001533508, 0.09850907325744629, 0.06217287853360176, 0.10980150103569031, 0.020894141867756844, 0.0717930793762207, 0.016278250142931938, 0.05087145045399666, 0.030110694468021393, 0.08057834208011627, 0.03867187723517418, 0.028333332389593124, 0.1235390231013298, 0.04383792355656624, 0.1814475804567337, 0.14631687104701996, 0.04316753149032593, 0.06620316207408905, 0.03670323267579079, 0.08946555107831955, 0.04376816004514694, 0.004936769139021635, 0.017047252506017685, 0.09794227033853531, 0.04746445640921593, 0.08174432069063187, 0.0398918017745018, 0.18266382813453674, 0.08081409335136414, 0.07309497147798538, 0.013806495815515518, 0.05747458338737488, 0.03830704838037491, 0.004765169229358435, 0.02878136746585369, 0.113812655210495, 0.03962098807096481, 0.028364254161715508, 0.085856594145298, 0.0400867722928524, 0.03267437592148781, 0.06541678309440613, 0.037479422986507416, 0.07082748413085938, 0.011786933057010174, 0.0494980663061142, 0.009744493290781975, 0.033364687114953995, 0.018633799627423286, 0.053487759083509445, 0.02178063988685608, 0.015951398760080338, 0.08801452815532684, 0.0519411675632, 0.24515530467033386, 0.18245647847652435, 0.07266883552074432, 0.10302911698818207, 0.0706978589296341, 0.14178642630577087, 0.07600027322769165, 0.011184078641235828, 0.03138827905058861, 0.13769571483135223, 0.0751183032989502, 0.13115079700946808, 0.06053301319479942, 0.2505689263343811, 0.12688879668712616, 0.11189184337854385, 0.02757054567337036, 0.09433268010616302, 0.06478185206651688, 0.011271805502474308, 0.1705874353647232, 0.3200494647026062, 0.19509954750537872, 0.13898847997188568, 0.1994510293006897, 0.14358851313591003, 0.13984933495521545, 0.20575684309005737, 0.15828798711299896, 0.23335173726081848, 0.08051499724388123, 0.16310378909111023, 0.06578006595373154, 0.12226990610361099, 0.10333941131830215, 0.1669306755065918, 0.13954220712184906, 0.10004569590091705, 0.20239166915416718, 0.06848768144845963, 0.06742007285356522, 0.07807887345552444, 0.011756463907659054, 0.026354346424341202, 0.010874268598854542, 0.030357131734490395, 0.014753410592675209, 0.0020424656104296446, 0.005783672910183668, 0.037526775151491165, 0.019052093848586082, 0.024299930781126022, 0.015852568671107292, 0.07191614806652069, 0.02725127525627613, 0.020765604451298714, 0.004898685961961746, 0.019362250342965126, 0.01465605292469263, 0.002164245117455721, 0.023162778466939926, 0.10586227476596832, 0.032175611704587936, 0.02214505895972252, 0.07862379401922226, 0.0310708936303854, 0.02459084428846836, 0.0573117733001709, 0.03093140386044979, 0.06255818158388138, 0.009662848897278309, 0.04702277109026909, 0.007748857140541077, 0.027554839849472046, 0.016658775508403778, 0.04643690213561058, 0.016755053773522377, 0.01301353145390749, 0.08062944561243057, 0.05514320731163025, 0.26947760581970215, 0.19557465612888336, 0.08242155611515045, 0.11638398468494415, 0.08187783509492874, 0.1683119684457779, 0.08647949993610382, 0.01262683980166912, 0.03216486796736717, 0.150259867310524, 0.08564399927854538, 0.15138667821884155, 0.06472573429346085, 0.2795777916908264, 0.15122663974761963, 0.11899780482053757, 0.02839965745806694, 0.1111079603433609, 0.0794040635228157, 0.011839566752314568, 0.06614448130130768, 0.20155999064445496, 0.08519858866930008, 0.06101642921566963, 0.12115579098463058, 0.06991642713546753, 0.06138857826590538, 0.11153411120176315, 0.07540944963693619, 0.1313522756099701, 0.030548173934221268, 0.0921703577041626, 0.02066049352288246, 0.057726651430130005, 0.043722979724407196, 0.09301178902387619, 0.05762546882033348, 0.03695554658770561, 0.13605526089668274, 0.04368419945240021, 0.14378215372562408, 0.12533670663833618, 0.03148782253265381, 0.05236676707863808, 0.029224859550595284, 0.07820720970630646, 0.0379379540681839, 0.004144720733165741, 0.011872759088873863, 0.07977066189050674, 0.04069188982248306, 0.06495930254459381, 0.03169676288962364, 0.1617988497018814, 0.07030002772808075, 0.04817390814423561, 0.010123217478394508, 0.05174580216407776, 0.038202520459890366, 0.004201901610940695, 0.07543884217739105, 0.22743140161037445, 0.09575797617435455, 0.0652063712477684, 0.13166023790836334, 0.07796258479356766, 0.06649713218212128, 0.12275929749011993, 0.08350390940904617, 0.14776673913002014, 0.03458290547132492, 0.0998920276761055, 0.02136804908514023, 0.06303973495960236, 0.04543115943670273, 0.10386191308498383, 0.06897074729204178, 0.03961808606982231, 0.14919085800647736, 0.04333340749144554, 0.128224179148674, 0.11915773898363113, 0.026265498250722885, 0.045638300478458405, 0.02351766638457775, 0.07017350941896439, 0.03304968401789665, 0.0030270759016275406, 0.008661622181534767, 0.07528962194919586, 0.0361887626349926, 0.05665875971317291, 0.02812843769788742, 0.1530960202217102, 0.06332747638225555, 0.036180444061756134, 0.007433481980115175, 0.044321659952402115, 0.03432176634669304, 0.0027279520872980356, 0.02893853932619095, 0.12668409943580627, 0.03351494297385216, 0.020188596099615097, 0.08990281820297241, 0.035490088164806366, 0.02339920587837696, 0.06307820975780487, 0.032127849757671356, 0.06677792221307755, 0.009861181490123272, 0.04479902237653732, 0.006310680415481329, 0.02630535513162613, 0.013535401783883572, 0.05734523758292198, 0.021065250039100647, 0.011724547483026981, 0.09430480748414993, 0.05336139723658562, 0.246914803981781, 0.18852299451828003, 0.07210557162761688, 0.09301958978176117, 0.06762300431728363, 0.15650375187397003, 0.07679644227027893, 0.008848872035741806, 0.02049691043794155, 0.14980299770832062, 0.07988676428794861, 0.13909542560577393, 0.05540425330400467, 0.27421239018440247, 0.14921768009662628, 0.08598720282316208, 0.018239589408040047, 0.09751393646001816, 0.0782824233174324, 0.0053018308244645596, 0.08831186592578888, 0.242971733212471, 0.09664871543645859, 0.05772250518202782, 0.13601316511631012, 0.07709816098213196, 0.06031334400177002, 0.12494640052318573, 0.07976619899272919, 0.13734105229377747, 0.03195077180862427, 0.08882883936166763, 0.018877509981393814, 0.0587797537446022, 0.037892140448093414, 0.11386747658252716, 0.06472302973270416, 0.03710262104868889, 0.14815378189086914, 0.04283573478460312, 0.12102906405925751, 0.12897183001041412, 0.027244802564382553, 0.042878977954387665, 0.020937930792570114, 0.07411851733922958, 0.028885172680020332, 0.002798181725665927, 0.006252825725823641, 0.07712263613939285, 0.03699742630124092, 0.0563807487487793, 0.027120480313897133, 0.1462397426366806, 0.06621594727039337, 0.0291247870773077, 0.005514323711395264, 0.04437914863228798, 0.03936626389622688, 0.0017016192432492971, 0.06173033267259598, 0.19783587753772736, 0.06561138480901718, 0.03496302291750908, 0.11753541231155396, 0.05392265319824219, 0.040216684341430664, 0.09880910813808441, 0.05680779367685318, 0.10140690207481384, 0.01994648389518261, 0.061441920697689056, 0.012356133200228214, 0.04281403496861458, 0.02447834610939026, 0.09030865877866745, 0.03898721560835838, 0.024020235985517502, 0.12555165588855743, 0.043014563620090485, 0.1632499098777771, 0.14771297574043274, 0.042512908577919006, 0.052836205810308456, 0.032676901668310165, 0.10206466913223267, 0.040396157652139664, 0.004284003749489784, 0.008899863809347153, 0.10297270119190216, 0.05170822888612747, 0.08121107518672943, 0.03553154692053795, 0.18922778964042664, 0.09327580779790878, 0.047428201884031296, 0.007802300620824099, 0.06047479063272476, 0.053993288427591324, 0.0022306283935904503, 0.04671066999435425, 0.16888375580310822, 0.049344345927238464, 0.024698453024029732, 0.106944240629673, 0.039503924548625946, 0.031259145587682724, 0.08298523724079132, 0.046529386192560196, 0.08824054896831512, 0.014481394551694393, 0.04792655259370804, 0.00905255414545536, 0.0368296355009079, 0.0183582566678524, 0.07425101846456528, 0.02592633292078972, 0.01745503954589367, 0.11260171979665756, 0.045002661645412445, 0.2021285742521286, 0.16024447977542877, 0.052813172340393066, 0.06132137402892113, 0.0438343845307827, 0.12602302432060242, 0.050242360681295395, 0.0052566989324986935, 0.010595419444143772, 0.1249866634607315, 0.061802592128515244, 0.10119333863258362, 0.04340613633394241, 0.22266238927841187, 0.1163473129272461, 0.06322178244590759, 0.009276783093810081, 0.07198141515254974, 0.06259103119373322, 0.0023771135602146387, 0.09153106808662415, 0.24911704659461975, 0.09450496733188629, 0.05291314795613289, 0.14083541929721832, 0.0633566603064537, 0.05756094306707382, 0.12757110595703125, 0.08618848770856857, 0.15274111926555634, 0.02987363189458847, 0.08381438255310059, 0.019018782302737236, 0.06525830179452896, 0.0377853587269783, 0.11230288445949554, 0.05333821475505829, 0.03620891273021698, 0.1544756293296814, 0.042082689702510834, 0.1323099434375763, 0.1170581802725792, 0.02569356933236122, 0.03208800405263901, 0.020141571760177612, 0.07374965399503708, 0.025882020592689514, 0.002256231615319848, 0.004823701456189156, 0.07820554822683334, 0.03630012646317482, 0.05114760994911194, 0.029424915090203285, 0.1431882679462433, 0.06580308079719543, 0.03186645731329918, 0.004242586903274059, 0.04012703150510788, 0.03639603406190872, 0.0011855100747197866, 0.04043129086494446, 0.15723463892936707, 0.04314280301332474, 0.022428041324019432, 0.10408134013414383, 0.03247770667076111, 0.028472723439335823, 0.08108242601156235, 0.046993009746074677, 0.09387952089309692, 0.01261233165860176, 0.04981236159801483, 0.007838854566216469, 0.0376642644405365, 0.018006902188062668, 0.0651288703083992, 0.02305515855550766, 0.015214961022138596, 0.11344917863607407, 0.047234971076250076, 0.2176184207201004, 0.15582548081874847, 0.056216683238744736, 0.05963188037276268, 0.048778336495161057, 0.13190612196922302, 0.0543757788836956, 0.004859875421971083, 0.0108739472925663, 0.12809863686561584, 0.06662096083164215, 0.10486331582069397, 0.050711456686258316, 0.23075103759765625, 0.12745067477226257, 0.06938828527927399, 0.0100548155605793, 0.07862569391727448, 0.06280729919672012, 0.002513645449653268, 0.04362094774842262, 0.16964726150035858, 0.04462184011936188, 0.026231633499264717, 0.11016874015331268, 0.03363194689154625, 0.03149406984448433, 0.0869874581694603, 0.05267349258065224, 0.1067582219839096, 0.013638488948345184, 0.05544190853834152, 0.008813115768134594, 0.040844421833753586, 0.020736560225486755, 0.0652565062046051, 0.029268423095345497, 0.01628243736922741, 0.1256340742111206, 0.04742209240794182, 0.2051296830177307, 0.14397549629211426, 0.048757899552583694, 0.05144789069890976, 0.04346316680312157, 0.11908377707004547, 0.05049899220466614, 0.003881308017298579, 0.009727969765663147, 0.11544991284608841, 0.06262648850679398, 0.09402395784854889, 0.050234198570251465, 0.22188514471054077, 0.11730135977268219, 0.061854880303144455, 0.00933554396033287, 0.07160350680351257, 0.055522311478853226, 0.0023340533953160048, 0.037415098398923874, 0.1463625133037567, 0.03774082660675049, 0.023643508553504944, 0.10668990761041641, 0.02866419404745102, 0.026927435770630836, 0.07677855342626572, 0.047691624611616135, 0.09779486805200577, 0.01098751462996006, 0.04762768745422363, 0.008404091000556946, 0.03573783114552498, 0.017984217032790184, 0.05417286977171898, 0.029400590807199478, 0.013657274655997753, 0.1171918511390686, 0.05389101430773735, 0.22192437946796417, 0.14827553927898407, 0.05555347353219986, 0.058256108313798904, 0.05321964994072914, 0.12716303765773773, 0.05928885564208031, 0.0048948233015835285, 0.012906966730952263, 0.12416569888591766, 0.06949325650930405, 0.10705220699310303, 0.05655648931860924, 0.2374131977558136, 0.1279670000076294, 0.06975502520799637, 0.012965623289346695, 0.07898563891649246, 0.05900648981332779, 0.002861445304006338, 0.05264347046613693, 0.17593075335025787, 0.055317703634500504, 0.035639043897390366, 0.120945043861866, 0.0391654409468174, 0.03643487021327019, 0.09445378929376602, 0.06499224156141281, 0.1241522803902626, 0.016501016914844513, 0.06173375993967056, 0.013513144105672836, 0.04538711532950401, 0.025221163406968117, 0.06681421399116516, 0.04515394568443298, 0.020736197009682655, 0.13820528984069824, 0.05058131739497185, 0.18124811351299286, 0.12608130276203156, 0.03780876100063324, 0.03943590819835663, 0.03629441559314728, 0.09150678664445877, 0.04357511177659035, 0.0031675344798713923, 0.008883163332939148, 0.0955791100859642, 0.05346548929810524, 0.07713422179222107, 0.04808884859085083, 0.19193974137306213, 0.09796208888292313, 0.051286835223436356, 0.009602573700249195, 0.06318429112434387, 0.04490716755390167, 0.0018194946460425854, 0.11232391744852066, 0.26938989758491516, 0.11218287795782089, 0.07893114537000656, 0.16890400648117065, 0.07701951265335083, 0.07277847081422806, 0.1494765430688858, 0.12663380801677704, 0.19988037645816803, 0.037317775189876556, 0.10576223582029343, 0.032279226928949356, 0.07904962450265884, 0.05311024561524391, 0.11399433016777039, 0.10055823624134064, 0.0480005107820034, 0.1937628537416458, 0.053051549941301346, 0.10381053388118744, 0.0856160819530487, 0.014830895699560642, 0.016953198239207268, 0.013952208682894707, 0.04128352180123329, 0.019728489220142365, 0.0012851196806877851, 0.003655229927971959, 0.054124344140291214, 0.027779778465628624, 0.032412365078926086, 0.030452709645032883, 0.11185251921415329, 0.04847514629364014, 0.02289251796901226, 0.003946743905544281, 0.033277057111263275, 0.02220926620066166, 0.0006843702285550535, 0.02009544149041176, 0.09512106329202652, 0.023516317829489708, 0.013759203255176544, 0.09105433523654938, 0.021451285108923912, 0.018304815515875816, 0.052133455872535706, 0.034797303378582, 0.06744718551635742, 0.00625696312636137, 0.029361093416810036, 0.00542196910828352, 0.024195291101932526, 0.011072784662246704, 0.037441547960042953, 0.017392991110682487, 0.008620131760835648, 0.09438330680131912, 0.0679599866271019, 0.28080570697784424, 0.18466435372829437, 0.07342980802059174, 0.09123533964157104, 0.07503978163003922, 0.16326452791690826, 0.08172406256198883, 0.007103117182850838, 0.01907510496675968, 0.16532766819000244, 0.09266000986099243, 0.150996595621109, 0.07733061164617538, 0.2963624596595764, 0.1681879311800003, 0.10457264631986618, 0.020512046292424202, 0.10927291214466095, 0.07649346441030502, 0.0025317054241895676, 0.039250243455171585, 0.13652923703193665, 0.041448935866355896, 0.02560923993587494, 0.11327450722455978, 0.03683890029788017, 0.03136679157614708, 0.07604247331619263, 0.057094186544418335, 0.09460445493459702, 0.011378785595297813, 0.043826039880514145, 0.00929331872612238, 0.038137249648571014, 0.017828181385993958, 0.05722678080201149, 0.029480013996362686, 0.015345095656812191, 0.1270533949136734, 0.05361974611878395, 0.21408116817474365, 0.1413215696811676, 0.04356342926621437, 0.054508186876773834, 0.04144544154405594, 0.10957604646682739, 0.04962079972028732, 0.0035189101472496986, 0.009661773219704628, 0.12003117799758911, 0.06639163941144943, 0.09759277105331421, 0.05534582585096359, 0.22334100306034088, 0.11743029952049255, 0.06781910359859467, 0.010138697922229767, 0.0753723531961441, 0.052974630147218704, 0.0010346815688535571, 0.03963872045278549, 0.13083645701408386, 0.039108406752347946, 0.023329615592956543, 0.11848674714565277, 0.039015039801597595, 0.03171811252832413, 0.07211530208587646, 0.05367269366979599, 0.08490656316280365, 0.009832409210503101, 0.039250973612070084, 0.0074956659227609634, 0.036446355283260345, 0.015111668966710567, 0.0544862262904644, 0.02647504396736622, 0.013220864348113537, 0.12677954137325287, 0.05594126880168915, 0.22090353071689606, 0.14314331114292145, 0.044612910598516464, 0.05454997345805168, 0.042089011520147324, 0.11674126237630844, 0.049270931631326675, 0.003607580903917551, 0.009693467989563942, 0.1232299655675888, 0.07297535985708237, 0.10606716573238373, 0.05506012961268425, 0.22358247637748718, 0.1262577772140503, 0.06415103375911713, 0.00983198918402195, 0.0756605863571167, 0.05548739433288574, 0.0008133030496537685, 0.06420617550611496, 0.18060681223869324, 0.058552537113428116, 0.040284398943185806, 0.1425473839044571, 0.05435320362448692, 0.04926152899861336, 0.09706508368253708, 0.07424288988113403, 0.1161217987537384, 0.015318574383854866, 0.055061060935258865, 0.009815631434321404, 0.05055548623204231, 0.022072579711675644, 0.07175798714160919, 0.04313181713223457, 0.019119152799248695, 0.15943174064159393, 0.04811260104179382, 0.170446515083313, 0.12123697251081467, 0.02625185437500477, 0.035476069897413254, 0.023423675447702408, 0.08401402831077576, 0.032752349972724915, 0.0019184694392606616, 0.005327298771589994, 0.09068062901496887, 0.05859312787652016, 0.07289636135101318, 0.04353556036949158, 0.16967512667179108, 0.09261047095060349, 0.036513254046440125, 0.004941397812217474, 0.050262678414583206, 0.04084526374936104, 0.0004005756927654147, 0.06266619265079498, 0.18994872272014618, 0.05696050077676773, 0.04163200780749321, 0.14510861039161682, 0.05673972889780998, 0.05307871848344803, 0.09852748364210129, 0.07138221710920334, 0.11530907452106476, 0.01539674773812294, 0.05578472092747688, 0.008946022018790245, 0.050725873559713364, 0.02265658974647522, 0.07225630432367325, 0.04695972055196762, 0.01885891519486904, 0.159419447183609, 0.046286068856716156, 0.1690048724412918, 0.12178763747215271, 0.023933889344334602, 0.03853098675608635, 0.020792093127965927, 0.08862713724374771, 0.03310009837150574, 0.0017248645890504122, 0.005129664670675993, 0.08804546296596527, 0.06007405370473862, 0.07754035294055939, 0.044729508459568024, 0.1675625741481781, 0.0943000465631485, 0.029901564121246338, 0.0041609033942222595, 0.04560960829257965, 0.03964974358677864, 0.0003186245739925653, 0.12000839412212372, 0.28233012557029724, 0.10791844874620438, 0.08468668907880783, 0.18481799960136414, 0.09254973381757736, 0.0913185179233551, 0.14568832516670227, 0.11895337700843811, 0.18079085648059845, 0.03560803830623627, 0.10431178659200668, 0.020271189510822296, 0.0813363566994667, 0.050196655094623566, 0.11503241956233978, 0.11998098343610764, 0.04344337806105614, 0.2062520533800125, 0.04243271425366402, 0.09606839716434479, 0.085320845246315, 0.009943793527781963, 0.02548614703118801, 0.00923486240208149, 0.048159487545490265, 0.0184621661901474, 0.0008531981147825718, 0.0028196233324706554, 0.0486503466963768, 0.038624271750450134, 0.045402925461530685, 0.034199319779872894, 0.10600416362285614, 0.05545304715633392, 0.010950671508908272, 0.0019979923963546753, 0.02270440198481083, 0.02209085412323475, 0.00023423721722792834, 0.027849163860082626, 0.11442144215106964, 0.022569673135876656, 0.016125861555337906, 0.10521043837070465, 0.038104187697172165, 0.030791668221354485, 0.056475624442100525, 0.03349946066737175, 0.052521057426929474, 0.00502322893589735, 0.026491736993193626, 0.0038443447556346655, 0.02557463012635708, 0.00983098242431879, 0.034490521997213364, 0.03348417207598686, 0.007868703454732895, 0.0961645171046257, 0.06374700367450714, 0.24611634016036987, 0.16625851392745972, 0.05427229031920433, 0.08158063143491745, 0.06459210067987442, 0.1715877205133438, 0.07657122611999512, 0.005990304984152317, 0.023412177339196205, 0.1456349939107895, 0.10625775158405304, 0.18586866557598114, 0.0707569345831871, 0.2568061649799347, 0.18330518901348114, 0.0582214780151844, 0.017703929916024208, 0.07349692285060883, 0.07094629108905792, 0.0015944184269756079, 0.08008313924074173, 0.22093181312084198, 0.06484688818454742, 0.05286451056599617, 0.1497529149055481, 0.0762919932603836, 0.07015503942966461, 0.11129684746265411, 0.08401969075202942, 0.1277492344379425, 0.017844201996922493, 0.07246282696723938, 0.013860059902071953, 0.057237595319747925, 0.02948932535946369, 0.0728086456656456, 0.10144919902086258, 0.026959750801324844, 0.15674734115600586, 0.044934097677469254, 0.12560006976127625, 0.09750428050756454, 0.019356951117515564, 0.03760545328259468, 0.023220282047986984, 0.08229909837245941, 0.03318164125084877, 0.0022137961350381374, 0.00897569116204977, 0.06413584202528, 0.05634543299674988, 0.09469673782587051, 0.046480342745780945, 0.14739450812339783, 0.09376353025436401, 0.018147272989153862, 0.0067417011596262455, 0.0318257138133049, 0.03240527585148811, 0.0007438180036842823, 0.03383077681064606, 0.1386016309261322, 0.027282940223813057, 0.019387682899832726, 0.10981663316488266, 0.04489576816558838, 0.03820805996656418, 0.06922953575849533, 0.0403888076543808, 0.06667247414588928, 0.006348473485559225, 0.035350531339645386, 0.005067225079983473, 0.031010471284389496, 0.011533859185874462, 0.038228556513786316, 0.049229465425014496, 0.010182322934269905, 0.1074923500418663, 0.057868592441082, 0.22876179218292236, 0.1471930891275406, 0.04511748626828194, 0.06133626773953438, 0.0576387383043766, 0.15139618515968323, 0.07100454717874527, 0.004690187517553568, 0.020082080736756325, 0.12332656234502792, 0.0934559553861618, 0.17296786606311798, 0.06846652179956436, 0.24276605248451233, 0.1640513837337494, 0.048693060874938965, 0.017188357189297676, 0.062413960695266724, 0.05950872600078583, 0.0014351680874824524, 0.07107660919427872, 0.2152944952249527, 0.0583178773522377, 0.04444066435098648, 0.1398889720439911, 0.06951403617858887, 0.06417616456747055, 0.1115749329328537, 0.07506528496742249, 0.1256743222475052, 0.017720967531204224, 0.07018987834453583, 0.013022984378039837, 0.053675588220357895, 0.030211329460144043, 0.06778495013713837, 0.10434131324291229, 0.024110477417707443, 0.15420646965503693, 0.04138520732522011, 0.14683203399181366, 0.09941324591636658, 0.019697539508342743, 0.031862981617450714, 0.02652650885283947, 0.0856667160987854, 0.04145511984825134, 0.0019393324619159102, 0.008666632696986198, 0.06887014210224152, 0.05614880844950676, 0.10154423862695694, 0.05193883553147316, 0.17266890406608582, 0.09230971336364746, 0.02720659412443638, 0.007717376109212637, 0.0392412468791008, 0.03515270724892616, 0.0007750311633571982, 0.08020664751529694, 0.22511334717273712, 0.06498740613460541, 0.052340392023324966, 0.14627790451049805, 0.07150158286094666, 0.07102997601032257, 0.11846619844436646, 0.08513592183589935, 0.14228475093841553, 0.02138424664735794, 0.07446447759866714, 0.01608908176422119, 0.059397004544734955, 0.03691527619957924, 0.07658424973487854, 0.12202733010053635, 0.027113093063235283, 0.16407707333564758, 0.038860831409692764, 0.1331360787153244, 0.09005889296531677, 0.016779957339167595, 0.025435663759708405, 0.0234965980052948, 0.07062749564647675, 0.03943713754415512, 0.0017393962480127811, 0.007781005930155516, 0.06251810491085052, 0.049169670790433884, 0.08960482478141785, 0.049441929906606674, 0.1658700704574585, 0.0759580135345459, 0.02892923541367054, 0.007653624285012484, 0.039220891892910004, 0.03189132362604141, 0.0008316506282426417, 0.08443133533000946, 0.2208731323480606, 0.06440800428390503, 0.057065073400735855, 0.14585301280021667, 0.07055303454399109, 0.06969822943210602, 0.11753091961145401, 0.08825819939374924, 0.14018410444259644, 0.021484822034835815, 0.06824707984924316, 0.017168620601296425, 0.0599246509373188, 0.03676658496260643, 0.07701953500509262, 0.1235811784863472, 0.025425471365451813, 0.16248497366905212, 0.038288794457912445, 0.12943480908870697, 0.08950106054544449, 0.01641547866165638, 0.025986431166529655, 0.023472150787711143, 0.06295933574438095, 0.03930973261594772, 0.0017746416851878166, 0.008052468299865723, 0.06377394497394562, 0.04472682625055313, 0.0823097750544548, 0.047145452350378036, 0.16231805086135864, 0.06361483782529831, 0.034835271537303925, 0.007591757923364639, 0.040072061121463776, 0.031065765768289566, 0.0009650618303567171, 0.0458136610686779, 0.15776851773262024, 0.03479662910103798, 0.03161686658859253, 0.11552955955266953, 0.04424494132399559, 0.041926078498363495, 0.0814700648188591, 0.05478504300117493, 0.08990361541509628, 0.010271204635500908, 0.03694283217191696, 0.007511813193559647, 0.03874850273132324, 0.018237102776765823, 0.054610706865787506, 0.07030182331800461, 0.011873654089868069, 0.12681005895137787, 0.045162029564380646, 0.19765937328338623, 0.12962761521339417, 0.03344931825995445, 0.04952175170183182, 0.03952857106924057, 0.10165626555681229, 0.0638623908162117, 0.0027176502626389265, 0.013247129507362843, 0.10918708890676498, 0.06684811413288116, 0.12181330472230911, 0.05987411364912987, 0.22092115879058838, 0.09348233789205551, 0.06526235491037369, 0.010977727361023426, 0.060037609189748764, 0.0485663041472435, 0.0013631656765937805, 0.03597318008542061, 0.13504397869110107, 0.029969608411192894, 0.026860550045967102, 0.10466086119413376, 0.03389173373579979, 0.03216921538114548, 0.07258683443069458, 0.045458048582077026, 0.07275435328483582, 0.009356865659356117, 0.030829235911369324, 0.005635072011500597, 0.02931072749197483, 0.016179068014025688, 0.053461953997612, 0.05295472592115402, 0.009784267283976078, 0.11376652866601944, 0.04625718295574188, 0.23439697921276093, 0.1536180078983307, 0.0486539825797081, 0.07110818475484848, 0.04410843551158905, 0.12125484645366669, 0.0782790333032608, 0.0032787788659334183, 0.016076043248176575, 0.13783372938632965, 0.07425765693187714, 0.1314207911491394, 0.06497570872306824, 0.2441990077495575, 0.10288304090499878, 0.08415800333023071, 0.010567104443907738, 0.07163774222135544, 0.06090433895587921, 0.0016537111951038241, 0.08864784240722656, 0.21884995698928833, 0.076015904545784, 0.06336282193660736, 0.142760768532753, 0.05656319856643677, 0.05585351958870888, 0.12498050183057785, 0.08858884871006012, 0.1243283674120903, 0.028540560975670815, 0.06188669428229332, 0.01492756512016058, 0.05168972164392471, 0.04192191734910011, 0.09928765892982483, 0.10942070186138153, 0.029112303629517555, 0.16126739978790283, 0.03339502587914467, 0.15249201655387878, 0.11867667734622955, 0.025489524006843567, 0.045415248721838, 0.016214480623602867, 0.06575515866279602, 0.041654523462057114, 0.0014760956401005387, 0.006445697508752346, 0.08584707230329514, 0.03825870156288147, 0.058884140104055405, 0.04164176061749458, 0.1549377739429474, 0.043387286365032196, 0.043977782130241394, 0.003539526369422674, 0.04168885946273804, 0.03418790176510811, 0.0007849039975553751, 0.05648979917168617, 0.16411300003528595, 0.046861689537763596, 0.037288203835487366, 0.11954489350318909, 0.041106928139925, 0.03539322316646576, 0.09343323111534119, 0.06147192791104317, 0.08609893918037415, 0.01483934000134468, 0.036505427211523056, 0.007361248135566711, 0.03432757034897804, 0.022092178463935852, 0.07784256339073181, 0.059573303908109665, 0.01766209490597248, 0.12947708368301392, 0.03540348634123802, 0.21163855493068695, 0.15011416375637054, 0.04397168755531311, 0.07004724442958832, 0.024991968646645546, 0.09735512733459473, 0.059131987392902374, 0.0025723669677972794, 0.009463929571211338, 0.12428336590528488, 0.05414225161075592, 0.08500991016626358, 0.05011987313628197, 0.20482182502746582, 0.06872493028640747, 0.07382329553365707, 0.005212389398366213, 0.06276541948318481, 0.051773689687252045, 0.0009887730702757835, 0.07982318103313446, 0.19175225496292114, 0.058622926473617554, 0.045169152319431305, 0.13168968260288239, 0.05160708725452423, 0.03965773805975914, 0.10741165280342102, 0.07410767674446106, 0.10325021296739578, 0.018374210223555565, 0.04472501948475838, 0.010007234290242195, 0.03966939449310303, 0.024681581184267998, 0.08677852898836136, 0.06744932383298874, 0.023415308445692062, 0.13982336223125458, 0.03365137055516243, 0.18931229412555695, 0.13730266690254211, 0.03450995683670044, 0.06138354539871216, 0.018834656104445457, 0.0862298309803009, 0.04632898047566414, 0.002288171323016286, 0.007014737464487553, 0.10652834177017212, 0.04440136253833771, 0.06369435787200928, 0.04066634550690651, 0.17287634313106537, 0.053459662944078445, 0.06432782113552094, 0.0041684238240122795, 0.054173264652490616, 0.0426713302731514, 0.0007379862945526838, 0.08755898475646973, 0.19195610284805298, 0.060157354921102524, 0.04330471530556679, 0.13144902884960175, 0.055098917335271835, 0.03747270628809929, 0.10615555197000504, 0.07497164607048035, 0.10072623938322067, 0.0181692223995924, 0.0436101108789444, 0.011162805370986462, 0.03600402921438217, 0.022982006892561913, 0.08571745455265045, 0.06240256503224373, 0.02625558152794838, 0.13254563510417938, 0.033828508108854294, 0.1847383826971054, 0.14156067371368408, 0.034200508147478104, 0.06207142397761345, 0.019136574119329453, 0.0892639011144638, 0.043610092252492905, 0.002673093229532242, 0.007112731225788593, 0.1044854149222374, 0.04491805285215378, 0.05783551558852196, 0.037004388868808746, 0.16546322405338287, 0.05415381118655205, 0.06470773369073868, 0.004612906835973263, 0.05616912990808487, 0.04341650754213333, 0.0007865977822802961, 0.0525767020881176, 0.12415595352649689, 0.03447526693344116, 0.022937260568141937, 0.09984247386455536, 0.03555537760257721, 0.019551752135157585, 0.06617767363786697, 0.04272112622857094, 0.05881202965974808, 0.00912912841886282, 0.024692175909876823, 0.006707960274070501, 0.017577743157744408, 0.011946790851652622, 0.053161922842264175, 0.03149857744574547, 0.014915550127625465, 0.08929277211427689, 0.042388636618852615, 0.2482706606388092, 0.17941054701805115, 0.06536756455898285, 0.09527352452278137, 0.044404253363609314, 0.13799738883972168, 0.06993214040994644, 0.007476346101611853, 0.016199257224798203, 0.1517351120710373, 0.07456515729427338, 0.09956679493188858, 0.05129252374172211, 0.231379434466362, 0.10025189816951752, 0.10766936838626862, 0.012134052813053131, 0.09168244898319244, 0.07051902264356613, 0.0018915602704510093, 0.08567991107702255, 0.16803978383541107, 0.05698705092072487, 0.0425134003162384, 0.11953970789909363, 0.05205904319882393, 0.031062457710504532, 0.0925668329000473, 0.06701407581567764, 0.0851631686091423, 0.016844699159264565, 0.04056213051080704, 0.012721061706542969, 0.029608335345983505, 0.021467724815011024, 0.06558047235012054, 0.052205078303813934, 0.024872537702322006, 0.11055419594049454, 0.03776981309056282, 0.1825902760028839, 0.14633220434188843, 0.042850229889154434, 0.06672109663486481, 0.030177399516105652, 0.09792466461658478, 0.04743252322077751, 0.005287182051688433, 0.011111744679510593, 0.11332767456769943, 0.05327809974551201, 0.06390213966369629, 0.038641419261693954, 0.17190687358379364, 0.06652699410915375, 0.07883474230766296, 0.009302235208451748, 0.06604456156492233, 0.05055661126971245, 0.0016006779624149203, 0.09998536109924316, 0.1951211392879486, 0.06624268740415573, 0.051974616944789886, 0.13420069217681885, 0.061480164527893066, 0.03864273801445961, 0.10629542917013168, 0.07933073490858078, 0.09990419447422028, 0.01886008493602276, 0.0473627932369709, 0.014216636307537556, 0.036903686821460724, 0.023862261325120926, 0.07213424146175385, 0.06208808720111847, 0.027422411367297173, 0.12563106417655945, 0.03741296008229256, 0.161418616771698, 0.133580282330513, 0.032227978110313416, 0.056669823825359344, 0.022959137335419655, 0.08181486278772354, 0.037554867565631866, 0.003497129073366523, 0.008189869113266468, 0.10063893347978592, 0.0457991287112236, 0.05234105885028839, 0.034270189702510834, 0.15175218880176544, 0.05581064894795418, 0.06293616443872452, 0.006960185244679451, 0.053248703479766846, 0.04283205792307854, 0.0010080039501190186, 0.04181341454386711, 0.11954919993877411, 0.028054064139723778, 0.021952899172902107, 0.09663988649845123, 0.03192553669214249, 0.018375148996710777, 0.060745857656002045, 0.04005490988492966, 0.051305901259183884, 0.006090348586440086, 0.02213132381439209, 0.005036691203713417, 0.01618364453315735, 0.008216884918510914, 0.039902906864881516, 0.02224487066268921, 0.010177290998399258, 0.08441251516342163, 0.04904066398739815, 0.2583247721195221, 0.1873030960559845, 0.06741664558649063, 0.09337475895881653, 0.05119753256440163, 0.1451089084148407, 0.06847333908081055, 0.0070147085934877396, 0.018226383253932, 0.16685160994529724, 0.08481182903051376, 0.11216387897729874, 0.05433275178074837, 0.23950308561325073, 0.11717003583908081, 0.10659473389387131, 0.014695289544761181, 0.09046463668346405, 0.07614226639270782, 0.0015237974002957344, 0.08938819169998169, 0.21056385338306427, 0.0574934221804142, 0.05020001158118248, 0.13652493059635162, 0.058789532631635666, 0.04077107086777687, 0.10514654964208603, 0.07834544777870178, 0.09765744209289551, 0.012898866087198257, 0.045724041759967804, 0.009436588734388351, 0.03696798160672188, 0.01787571609020233, 0.06895888596773148, 0.058785684406757355, 0.02038206160068512, 0.1334776133298874, 0.038301706314086914, 0.17436134815216064, 0.1433132290840149, 0.02893916517496109, 0.05804191157221794, 0.020321087911725044, 0.08460284024477005, 0.0331406444311142, 0.002167440252378583, 0.006578291766345501, 0.10204355418682098, 0.04927854984998703, 0.05620991438627243, 0.03574739769101143, 0.15503092110157013, 0.06044510379433632, 0.0548049621284008, 0.005231643095612526, 0.047366395592689514, 0.041463032364845276, 0.0004918976919725537, 0.04705483838915825, 0.14528028666973114, 0.029000304639339447, 0.026903176680207253, 0.10245321691036224, 0.03638416901230812, 0.02373390644788742, 0.06679702550172806, 0.048131633549928665, 0.06137152761220932, 0.005952199921011925, 0.02655339054763317, 0.004224355332553387, 0.021981632336974144, 0.00916923675686121, 0.044219449162483215, 0.031608082354068756, 0.01000304613262415, 0.10153535008430481, 0.04417857155203819, 0.24656224250793457, 0.18225659430027008, 0.05451301112771034, 0.0815313532948494, 0.04021153226494789, 0.12332867830991745, 0.05708356574177742, 0.004427433479577303, 0.012935646809637547, 0.15044645965099335, 0.07846085727214813, 0.10121744871139526, 0.04983719065785408, 0.22282922267913818, 0.10789637267589569, 0.09543641656637192, 0.010208983905613422, 0.07753276079893112, 0.06639593094587326, 0.0008048565359786153, 0.1657654196023941, 0.3072894811630249, 0.1206226721405983, 0.09712259471416473, 0.18592281639575958, 0.10812993347644806, 0.08291779458522797, 0.16538013517856598, 0.1479511559009552, 0.1788107007741928, 0.033156417310237885, 0.09256845712661743, 0.02304590679705143, 0.08301406353712082, 0.04691319540143013, 0.11069311946630478, 0.10912957042455673, 0.05468503013253212, 0.17850752174854279, 0.040626369416713715, 0.10802318155765533, 0.09639498591423035, 0.010925982147455215, 0.0316997766494751, 0.007531737443059683, 0.03997936472296715, 0.01452747080475092, 0.0008937186794355512, 0.0026826215907931328, 0.054518960416316986, 0.026385463774204254, 0.023666316643357277, 0.01998681202530861, 0.08229821175336838, 0.032431066036224365, 0.030279532074928284, 0.0022823482286185026, 0.02759190835058689, 0.020437970757484436, 0.00022816321870777756, 0.07246885448694229, 0.19802629947662354, 0.05497611686587334, 0.039820678532123566, 0.12100537121295929, 0.06200521066784859, 0.042810987681150436, 0.09918948262929916, 0.08033078908920288, 0.10278891026973724, 0.011864880099892616, 0.04972093924880028, 0.008787384256720543, 0.04288376122713089, 0.017861196771264076, 0.06440547853708267, 0.03368619084358215, 0.02482352964580059, 0.11769525706768036, 0.03040977567434311, 0.2023821324110031, 0.13787811994552612, 0.025610942393541336, 0.053595542907714844, 0.020360780879855156, 0.08926726132631302, 0.030013980343937874, 0.002332874108105898, 0.0063309380784630775, 0.10194741189479828, 0.05176509916782379, 0.05764665827155113, 0.032984841614961624, 0.14997580647468567, 0.08081530779600143, 0.07169107347726822, 0.006062903441488743, 0.061670973896980286, 0.04453611746430397, 0.00043097991147078574, 0.05983702465891838, 0.1813274621963501, 0.0475480780005455, 0.033968616276979446, 0.10806271433830261, 0.05412076413631439, 0.04075911268591881, 0.08552834391593933, 0.07401898503303528, 0.09925928711891174, 0.011582336388528347, 0.04884827882051468, 0.00821622833609581, 0.03814253211021423, 0.017580555751919746, 0.06216355785727501, 0.028375675901770592, 0.023806104436516762, 0.10759087651968002, 0.030549926683306694, 0.2144417017698288, 0.1395019292831421, 0.029639558866620064, 0.05759472772479057, 0.025751452893018723, 0.09694843739271164, 0.0339159220457077, 0.0031577832996845245, 0.007549516391009092, 0.10763490200042725, 0.056632909923791885, 0.06728395819664001, 0.035069286823272705, 0.16042734682559967, 0.09209118783473969, 0.08188190311193466, 0.008249365724623203, 0.06972838938236237, 0.04926735535264015, 0.0007109481957741082, 0.09718429297208786, 0.23205776512622833, 0.07285541296005249, 0.05949917808175087, 0.1312871128320694, 0.0740644782781601, 0.06404942274093628, 0.11009883880615234, 0.10780912637710571, 0.14108379185199738, 0.022203834727406502, 0.07298740744590759, 0.015261051245033741, 0.054660189896821976, 0.03217003494501114, 0.08794001489877701, 0.05450098589062691, 0.04151102155447006, 0.13796719908714294, 0.03104366920888424, 0.1544758677482605, 0.10759332776069641, 0.018235474824905396, 0.03975965455174446, 0.015824727714061737, 0.06424670666456223, 0.0212780199944973, 0.0020932925399392843, 0.004702388774603605, 0.07617130875587463, 0.04056812822818756, 0.04214831069111824, 0.025648241862654686, 0.11462964117527008, 0.059903450310230255, 0.058918438851833344, 0.005342565476894379, 0.050484172999858856, 0.033195655792951584, 0.000646617729216814]\n"
     ]
    }
   ],
   "source": [
    "## TODO: Use a pretrained model to classify the cat and dog images\n",
    "## Don't forget to use model.train() and model.eval()\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.classifier.parameters(), lr = 0.003)\n",
    "epochs = 1\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"using device {device}\")\n",
    "model.to(device)\n",
    "\n",
    "running_loss = 0\n",
    "count = 0\n",
    "test_loss = []\n",
    "\n",
    "for e in range(1, epochs + 1):\n",
    "    running_loss = 0\n",
    "    model.train()\n",
    "    for images, labels in trainloader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        \n",
    "        log_ps = model.forward(images)\n",
    "        loss = criterion(log_ps, labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        count = count + 1\n",
    "        \n",
    "        if count % 5 == 0:\n",
    "            test_acc = 0\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "\n",
    "                for images, labels in testloader:\n",
    "                    images = images.to(device)\n",
    "                    labels = labels.to(device)\n",
    "\n",
    "                    log_ps = model.forward(images)\n",
    "                    test_loss.append(criterion(log_ps, labels).item())\n",
    "                    \n",
    "                    ps = torch.exp(log_ps)\n",
    "                    top_ps, top_class = ps.topk(1, dim = 1)\n",
    "                    equal = top_class == labels.view(*top_class.shape)\n",
    "                    test_acc += torch.mean(equal.type(torch.FloatTensor))\n",
    "                test_acc /= len(testloader)\n",
    "\n",
    "            print(f'The {e}-th epochs')\n",
    "            print(f'  running loss: {running_loss}')\n",
    "            print(f'  test_accuracy: {test_acc.item() * 100} %')\n",
    "print(\"test loss\", test_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
